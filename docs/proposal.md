# 基于可扩展基数树（ART）的键值内存数据库设计与实现

## 一、课题背景与研究意义

### 1.1 研究背景

Key-Value 内存数据库因其 **低延迟、高吞吐** 的特性，被广泛应用于缓存系统、在线服务、实时分析等场景。典型代表包括 Redis、Memcached 等。这类系统通常将全部或大部分数据存储在内存中，以牺牲部分持久性换取极致性能。

随着业务规模扩大与硬件环境演进，传统内存数据库在以下方面逐渐暴露出瓶颈：

- **多核 CPU 资源利用率不足**  
  早期内存数据库多采用单线程模型（如 Redis），在 IO 性能有限、数据规模较小时效果良好。但在现代多核环境下，该模型难以充分利用硬件并行能力。

- **索引结构性能受限**  
  当前主流内存数据库多采用：
  - 哈希表（不支持范围查询）
  - 跳表（缓存不友好、内存利用率较低）  
  索引结构往往成为系统性能上限的决定因素。

- **新硬件加速趋势出现**  
  随着 GPU 可编程性增强，利用 GPU 加速数据库索引查询成为研究热点，但其在通用内存数据库中的应用仍面临架构与开销挑战。

### 1.2 研究意义

本课题围绕 **索引结构优化 + 并发控制 + 持久化 + GPU 加速探索** 展开，具有以下意义：

- 为 **现代多核架构下的内存数据库设计** 提供实践参考
- 验证 **可扩展基数树（ART）** 在内存 KV 场景中的工程可行性
- 探索 **GPU 批量查询加速** 在真实数据库架构中的适用边界
- 为后续高性能内存数据库或研究型系统提供可复用的设计经验

---

## 二、问题分析与技术挑战

### 2.1 索引结构选择问题

索引结构直接决定数据库的：

- 查询复杂度
- 缓存友好性
- 内存占用
- 并发扩展能力

传统结构存在如下不足：

| 索引结构 | 优点 | 局限 |
|--------|------|------|
| 哈希表 | O(1) 点查 | 不支持范围查询 |
| 跳表 | 支持范围 | 指针多、cache miss 高 |
| B+ 树 | 有序、稳定 | 更新代价高、内存放大 |

### 2.2 并发控制挑战

内存数据库在高并发场景下需同时满足：

- 高吞吐
- 读写不互相阻塞
- 数据一致性

简单锁机制在高并发下容易成为瓶颈，因此需要更精细的并发控制策略。

### 2.3 持久化与崩溃恢复

尽管以内存为主，数据库仍需保证：

- 崩溃后数据可恢复
- 写入具备一定持久性语义

如何在 **不显著影响性能** 的前提下实现持久化，是内存数据库的核心工程问题。

### 2.4 GPU 加速的现实约束

GPU 在索引查询中面临：

- 显存随机访问性能差
- CPU-GPU 数据拷贝开销高
- 不适合细粒度、低并发请求

因此 GPU 加速需要严格限定适用场景。

---

## 三、总体设计方案

### 3.1 系统总体目标

设计并实现一个：

- 基于 **ART 索引结构**
- 支持 **高并发访问**
- 具备 **WAL + Snapshot 持久化**
- 并探索 **GPU 查询加速**

的键值内存数据库原型系统。

---

## 四、核心技术方案

## 4.1 索引结构设计：Adaptive Radix Tree（ART）

### 4.1.1 ART 基本原理

ART 是一种基于前缀的有序索引结构，具有以下特性：

- 点查询复杂度：O(k)，k 为 key 长度
- 树高与数据量无关
- 不需要树平衡操作
- 键值按字典序天然有序
- 无需完整存储 key

### 4.1.2 节点类型设计

ART 根据子节点数量动态选择节点类型：

| 节点类型 | 子节点上限 | 特点 |
|--------|------------|------|
| Node4  | 4  | 紧凑、cache 友好 |
| Node16 | 16 | SIMD 并行比较 |
| Node48 | 48 | 索引映射 |
| Node256 | 256 | 直接数组索引 |

- 小节点（Node4 / Node16）使用 **SIMD 指令** 加速 key byte 匹配
- 大节点采用数组或二分方式进行子节点定位

### 4.1.3 路径压缩（Path Compression）

- 使用 `prefix + prefix_len` 压缩单分支路径
- 插入 / 删除时可能触发 prefix 拆分或合并
- 显著降低树高，提高 cache 命中率

### 4.1.4 并发访问：ROWEX 协议

- 读操作无锁
- 写操作加锁
- 通过原子指针替换保证读一致性
- 避免读线程读取到不完整结构

---

## 4.2 并发控制：MVCC + OCC

### 4.2.1 并发控制模型

采用 **多版本并发控制（MVCC）**，结合 **乐观并发控制（OCC）**：

- 写不阻塞读
- 支持 time-travel 查询（扩展性）

### 4.2.2 事务执行流程

1. **Read Phase**
   - 读取数据版本
   - 构建私有读写集

2. **Validation Phase**
   - 校验是否与其他事务冲突

3. **Write Phase**
   - 验证成功则提交
   - 否则回滚并重试

### 4.2.3 版本存储与回收

- 版本按 **新到旧（N2O）** 链表存储
- Append-Only 物理布局
- 采用 **Epoch-Based Reclamation（EBR）**
  - 延迟回收旧版本
  - 避免悬垂指针问题

---

## 4.3 持久化与崩溃恢复

### 4.3.1 WAL（Write-Ahead Logging）

- 所有写操作先写 WAL，再执行内存更新
- 顺序写磁盘，降低 IO 成本
- 支持：
  - CRC 校验
  - Group Commit
  - Sync / Non-sync 两种模式

### 4.3.2 Snapshot 机制

- 后台线程周期性触发
- 使用 `fork()` + Copy-on-Write
- 快照完成后清理旧 WAL 文件

### 4.3.3 崩溃恢复流程

1. 启动时加载最近 Snapshot
2. 使用 ART 的批量加载接口构建索引
3. 回放 Snapshot 之后的 WAL 日志

---

## 4.4 GPU 查询加速（探索性）

### 4.4.1 设计思路

参考 CuART 论文思想：

- 节点类型分离存储
- 面向只读查询
- 批量请求统一下发 GPU

### 4.4.2 GPU 执行模型

- One-query-per-thread
- 仅加速批量点查 / 范围查
- 避免频繁 CPU-GPU 数据传输

### 4.4.3 实验评估

- 不同 batch size 对比
- CPU / GPU 吞吐量对比
- P99 延迟分析

---

## 五、系统技术架构

### 5.1 分层架构设计

- **接口层**：支持 Redis 协议
- **执行层**：CPU-GPU 调度器
- **索引层**：CPU / GPU 双端 ART
- **内存管理层**：Epoch 内存回收
- **持久化层**：WAL + Snapshot

### 5.2 技术选型

- 开发语言：C++20
- GPU 编程：NVIDIA CUDA
- 运行环境：Linux

---

## 六、计划进度安排

| 时间 | 任务 |
|----|----|
| 2025.11 – 2025.12 | 文献调研与架构设计 |
| 2025.12 – 2026.01 | 内存管理与回收机制 |
| 2026.01 – 2026.02 | ART 与 GPU 优化 |
| 2026.02 – 2026.03 | WAL 与恢复机制 |
| 2026.03 – 2026.04 | 性能测试与优化 |
| 2026.04 – 2026.05 | 论文撰写与答辩 |

---

## 七、总结

本课题以 **ART 索引结构** 为核心，系统性地探讨了现代内存数据库在 **并发控制、索引优化、持久化与硬件加速** 等方面的关键问题，具有明确的工程落地目标与研究价值，为后续高性能数据库系统设计提供了可借鉴的实现路径。

